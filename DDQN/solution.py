


## To Implement in DDQN agent
def learn(self):

    if self.memory.mem_cntr < self.batch_size:
        return
    

    self.q_eval.optimizer.zero_grad()

    self.replace_target_network()

    states, actions, rewards, states_, dones = self.sample_memory()

    indices = np.arange(self.batch_size)

    q_pred = self.q_eval.forward(states)[indices, actions]

    q_next = self.q_next.forward(states_)

    q_eval = self.q_eval.forward(states_)

    max_actions = T.argmax(q_eval, dim=1)

    q_next[dones] = 0.0

    q_target = rewards + self.gamma * q_next[indices, max_actions]

    loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)
    loss.backward()


    self.q_eval.optimizer.step()
    self.learn_step_counter += 1
    self.decrement_epsilon()
